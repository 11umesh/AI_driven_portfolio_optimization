# -*- coding: utf-8 -*-
"""lstmipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_PNFOhLBfcCz8CEeqr3s6ucKkIFby3O
"""

import yfinance as yf
import pandas as pd

# Define tickers and date range
tickers = ['^GSPC', '^FTSE', '^N225', 'EEM']
start_date = '2010-01-01'
end_date = '2020-12-31'

# Download adjusted close prices
df_eq = yf.download(tickers, start=start_date, end=end_date)#['Adj Close']
#data.to_csv("market_data.csv")

df_eq

pip install investpy

import yfinance as yf

# Download Gold Futures (XAU/USD proxy)
df_g = yf.download('GC=F', start='2010-01-01', end='2020-12-31')#['Adj Close']
df_g.name = 'Gold_XAUUSD'
#gold.to_csv('gold_price_yahoo.csv')

df_g

"""import pandas_datareader.data as web

ust_10y = web.DataReader('GS10', 'fred', start_date, end_date)
ust_10y.to_csv('us_10y_bond.csv')
print("Downloaded 10Y Treasury:\n", ust_10y.head())
"""

df_gb = yf.download('^TNX', start='2010-01-01', end='2020-12-31')#['Adj Close']
df_gb = df_gb / 10  # Convert from basis points to percentage (e.g., 45.00 -> 4.5%)
df_gb.name = '10Y Treasury Yield (%)'

df_gb

"""df_eq.columns = df_eq.columns.get_level_values(0)
df_eq.columns
"""

dff = pd.concat([df_eq, df_g, df_gb], axis=1)

dff

df_combined = dff[['Close', 'Volume']]

df_combined

df_combined.isna().sum()

df_returns = pd.DataFrame()
for col in df_combined.columns:
  if 'Close' in col:
    # Calculate daily log return
    df_returns[f'{col[1]}_lr'] = np.log(df_combined[col] / df_combined[col].shift(1))

df_returns

import numpy as np

# Extract Volume columns from df_combined
volume_cols = df_combined.filter(like='Volume')

# Add Volume columns to df_returns
for col in volume_cols.columns:
  df_returns[f'vol_{col[1]}'] = volume_cols[col]

df_returns

df_returns.dropna(inplace=True)
df_returns

df_returns = df_returns.drop(columns=['vol_^TNX'])

for col in df_returns.filter(like='_lr').columns:
  df_returns[f'{col}_5d_maret'] = df_returns[col].rolling(window=5).mean()
  df_returns[f'{col}_21d_maret'] = df_returns[col].rolling(window=21).mean()
  df_returns[f'{col}_21d_volty'] = df_returns[col].rolling(window=21).std()

df_returns

df_returns.dropna(inplace=True)
df_returns

train_data = df_returns[df_returns.index.year <= 2018]
test_data = df_returns[df_returns.index.year > 2018]

print("Train data shape:", train_data.shape)
print("Test data shape:", test_data.shape)

from sklearn.preprocessing import StandardScaler

# Identify features to standardize (all columns except potentially the target if it existed)
features_to_standardize = train_data.columns

# Initialize StandardScaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both train and test data
train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data),index=train_data.index)

test_data_scaled = pd.DataFrame(scaler.transform(test_data),index=test_data.index)

test_data_scaled

def create_lstm_sequences(data, lookback=60):
    X, y = [], []
    for i in range(lookback, len(data)-1):
        X.append(data.iloc[i-lookback:i].values)
        y.append(data.iloc[i+1].values)
    return np.array(X), np.array(y)


X_train, y_train = create_lstm_sequences(train_data_scaled, lookback=60)
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

X_test, y_test = create_lstm_sequences(test_data_scaled, lookback=60)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks

model = models.Sequential([
    layers.Input(shape=(X.shape[1], X.shape[2])),
    layers.LSTM(50, return_sequences=True),
    layers.Dropout(0.2),
    layers.LSTM(50),
    layers.Dropout(0.2),
    layers.Dense(y.shape[1])  # Multi-output regression
])

model.compile(optimizer=tf.keras.optimizers.Adam(0.001),
              loss='mse')

# Early stopping
early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train,
                    validation_split=0.1,
                    epochs=100,
                    batch_size=32,
                    callbacks=[early_stop],
                    verbose=1)

from sklearn.metrics import mean_squared_error, r2_score,root_mean_squared_error

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = root_mean_squared_error(y_test, y_pred)

print(f"Test MSE: {mse:.4f}")
print(f"R² Score: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")

"""transformer"""

import tensorflow as tf
from tensorflow.keras import layers, models, regularizers

def transformer_encoder(inputs, num_heads=8, model_dim=64, ff_dim=128, dropout=0.1):
    # Multi-head Self Attention
    attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=model_dim)(inputs, inputs)
    attention = layers.Dropout(dropout)(attention)
    attention = layers.LayerNormalization(epsilon=1e-6)(inputs + attention)

    # Feedforward network
    ff = layers.Dense(ff_dim, activation='relu')(attention)
    ff = layers.Dense(model_dim)(ff)
    ff = layers.Dropout(dropout)(ff)
    out = layers.LayerNormalization(epsilon=1e-6)(attention + ff)

    return out

X_train.shape,y_train.shape[1]

x

sequence_length = X_train.shape[1]
num_features = X_train.shape[2]
num_outputs = y_train.shape[1]
model_dim = 64

inputs = layers.Input(shape=(sequence_length, num_features))  # (60, 39)

# Project inputs from 39 → 64
x = layers.Dense(model_dim)(inputs)  # Now x is (60, 64)

# Positional Encoding
positions = tf.range(start=0, limit=sequence_length, delta=1)
positional_encoding = layers.Embedding(input_dim=sequence_length, output_dim=model_dim)(positions)
x = x + positional_encoding  # Broadcast add: shape (60, 64)

# Transformer Encoder Blocks
x = transformer_encoder(x)
x = transformer_encoder(x)

# Flatten and Dense output
x = layers.Flatten()(x)
outputs = layers.Dense(num_outputs)(x)

# Model compile
transformer_model = models.Model(inputs=inputs, outputs=outputs)
transformer_model.compile(optimizer=tf.keras.optimizers.Adam(0.001),
                          loss='mse')
transformer_model.summary()

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = transformer_model.fit(
    X_train, y_train,
    validation_split=0.1,
    epochs=100,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

y_pred_transformer = transformer_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred_transformer)
r2 = r2_score(y_test, y_pred_transformer)
rmse = root_mean_squared_error(y_test, y_pred_transformer)

print("Transformer Test MSE:", round(mse, 4))
print(f"Transformer R² Score: {r2:.4f}")
print(f"Transformer RMSE: {rmse:.4f}")

"""baseline model"""

X_train_ridge = X_train.reshape((X_train.shape[0], -1))
X_test_ridge  = X_test.reshape((X_test.shape[0], -1))

from sklearn.linear_model import Ridge
from sklearn.multioutput import MultiOutputRegressor

# Ridge model with alpha = 1.0
ridge = MultiOutputRegressor(Ridge(alpha=1.0))
ridge.fit(X_train_ridge, y_train)

y_pred_ridge = ridge.predict(X_test_ridge)

# Overall RMSE
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2 = r2_score(y_test, y_pred_ridge)
rmse = root_mean_squared_error(y_test, y_pred_ridge)

print("Ridge Test MSE:", round(mse_ridge, 4))
print(f"Ridge R² Score: {r2:.4f}")
print(f"Ridge RMSE: {rmse:.4f}")

# for i in range(y_test.shape[1]):
#     rmse_i = np.sqrt(mean_squared_error(y_test[:, i], y_pred_ridge[:, i]))
#     print(f"Asset {i} (Ridge): RMSE = {rmse_i:.4f}")

"""PO"""

pip install PyPortfolioOpt

from pypfopt import EfficientFrontier, risk_models, expected_returns

# Convert test set predictions and actual returns to DataFrames
dates = test_data.index[-len(y_test):]
preds = pd.DataFrame(y_pred_transformer[:,:6], index=dates, columns=test_data.columns[:6])
actuals = pd.DataFrame(y_test[:,:6], index=dates, columns=test_data.columns[:6])
prices_test = np.exp(actuals.cumsum())

actuals

portfolio_value = [100]
weights_record = []

for i in range(60, len(preds)-1):
    pred_return = preds.iloc[i]
    window_returns = actuals.iloc[i-60:i]  # past returns for covariance

    # Estimate covariance
    cov_matrix = window_returns.cov()

    # Portfolio Optimization
    try:
        ef = EfficientFrontier(pred_return, cov_matrix)
        weights = ef.max_sharpe()
        cleaned_weights = ef.clean_weights()
        weights_vec = np.array([cleaned_weights.get(asset, 0) for asset in actuals.columns])
    except:
        weights_vec = np.ones(len(pred_return)) / len(pred_return)  # fallback to equal weight

    # Simulate next-day return
    next_return = actuals.iloc[i+1].values
    daily_portfolio_return = np.dot(weights_vec, next_return)
    new_value = portfolio_value[-1] * np.exp(daily_portfolio_return)  # log return
    portfolio_value.append(new_value)
    weights_record.append(weights_vec)

portfolio_value_df = pd.DataFrame(portfolio_value,index=dates[-len(portfolio_value):])
portfolio_value_df



import matplotlib.pyplot as plt

plt.plot(portfolio_value)
plt.title("AI-driven Portfolio Value Over Time")
plt.ylabel("Portfolio Value ($)")
plt.xlabel("Days")
plt.grid(True)
plt.show()

"""ew"""

equal_weight = np.ones(actuals.shape[1]) / actuals.shape[1]

ew_returns = actuals @ equal_weight
ew_cum_value = np.exp(np.cumsum(ew_returns)) * 100  # Starting from $100

actuals

ew_returns

ew_cum_value

"""static mvo"""

from pypfopt import EfficientFrontier

# Estimate mean and cov from training set
mu_train = pd.DataFrame(y_train[:,:6], columns=actuals.columns).mean()
cov_train = pd.DataFrame(y_train[:,:6], columns=actuals.columns).cov()

# Solve for tangency portfolio (max Sharpe)
ef_static = EfficientFrontier(mu_train, cov_train)
weights_static = ef_static.max_sharpe()
weights_static = np.array(list(weights_static.values()))

# Apply static weights daily
static_returns = actuals @ weights_static
static_cum_value = np.exp(np.cumsum(static_returns)) * 100

static_cum_value

plt.figure(figsize=(10, 6))
plt.plot(ew_cum_value, label='Equal Weight')
plt.plot(static_cum_value, label='Static MVO')
plt.plot(portfolio_value_df, label='AI Portfolio (Transformer)')
plt.title("Portfolio Value Comparison")
plt.ylabel("Value ($)")
plt.xlabel("Days")
plt.legend()
plt.grid(True)
plt.show()

ai_values = np.array(portfolio_value)
ew_values = 100 * np.exp(np.cumsum(ew_returns))
static_values = 100 * np.exp(np.cumsum(static_returns))

#daily log returns for each, log ret
ai_returns_l = np.diff(np.log(ai_values))
ew_returns_l = np.diff(np.log(ew_values))
static_returns_l = np.diff(np.log(static_values))

def evaluate_performance(returns, name="Strategy"):
    annual_return = np.exp(np.mean(returns) * 252) - 1
    annual_vol = np.std(returns) * np.sqrt(252)
    sharpe = annual_return / annual_vol if annual_vol != 0 else 0

    # Max drawdown
    cumulative = np.exp(np.cumsum(returns))
    peak = np.maximum.accumulate(cumulative)
    drawdown = (cumulative - peak) / peak
    max_dd = drawdown.min()
    calmar = annual_return / abs(max_dd) if max_dd != 0 else 0

    print(f"\n📈 {name}")
    print(f"  Annual Return:   {annual_return:.2%}")
    print(f"  Volatility:      {annual_vol:.2%}")
    print(f"  Sharpe Ratio:    {sharpe:.2f}")
    print(f"  Max Drawdown:    {max_dd:.2%}")
    print(f"  Calmar Ratio:    {calmar:.2f}")

    return {
        "Return": annual_return,
        "Volatility": annual_vol,
        "Sharpe": sharpe,
        "Drawdown": max_dd,
        "Calmar": calmar
    }

perf_ai = evaluate_performance(ai_returns_l, "AI (Transformer)")
perf_ew = evaluate_performance(ew_returns_l, "Equal Weight")
perf_static = evaluate_performance(static_returns_l, "Static MVO")

